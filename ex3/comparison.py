# from plotnine import *
from ex3iml.models import *


def f(X):
    """
    true hypothesis for the labels to the given samples X - m x 2 matrix
    :param X: normal distributed samples
    :return: vector for the true labels of X
    """
    w, b = np.array([0.3, -0.5]), 0.1
    f = lambda x: np.sign((w.T @ x.T) + b)
    return f(X)

def draw_points(m):
    """
    given an integer m returns a pair X,y where X is m x 2 matrix where each
    column represents an i.i.d sample from the distribution above
    and y from {-1,1}^m is its corresponding label, according to f(x)
    :param m: integer
    :return: pair X,y
    """
    mean, cov = np.array([0, 0]), np.array([[1, 0], [0, 1]])
    x_samples = np.random.multivariate_normal(mean, cov, m)
    # print(x_samples)

    # w, b = np.array([0.3, -0.5]), 0.1
    # f = lambda x: np.sign((w.T @ x.T) + b)
    # y = f(x_samples)
    y = f(x_samples)
    return x_samples, y

def create_samples_and_models():
    """
    draw for each m (training points) points {x1,.,xm} the following figure
    • The drawn data points, colored according to their labels (e.g., blue for positive labels
    and orange for negative ones)
    • Add the hyperplane of the true hypothesis (the function f)
    • Add the hyperplane of the hypothesis generated by the perceptron
    • Add the hyperplane of the hypothesis generated by SVM
    • Add a legend to explain which hyperplane is which

    """
    m = np.array([5, 10, 15, 25, 70])
    for i in m:
        # remark for myself: recall that for a model (w, b) where b is the intercept, the hyperplane
        # it the project of w, meaning, for any new data sample xi, we will project
        # xi w, add b and see the sign we got with the multifiction of 1, and -1 with
        # the goal of getting yi<w,xi> > 0 always.
        # in our case we have 2 labels {-1, +1} so we can represent this hyperplane
        # by a line s.t each side (up, down) of the line represent the classification we gave
        # for each algorithm with hope the real data "agrees" with the side we gave
        x_samples, y = draw_points(i)
        label_y1, label_y_minus1 = x_samples[y == 1, :], x_samples[y == -1, :]
        w, b = np.array([0.3, -0.5]), 0.1
        domain_x = np.linspace(np.amin(x_samples), np.max(x_samples))


        true_hyperplane = ((-w[0] / w[1]) * domain_x) - (b / w[1])

        perceptron = Perceptron()
        perceptron.fit(x_samples, y)
        perceptron_model = perceptron.model
        perceptron_intercept = perceptron_model[0]
        perceptron_hyperplane = ((-perceptron_model[1] / perceptron_model[2]) * domain_x)\
                                - (perceptron_intercept/ perceptron_model[2])



        svm = SVM()
        svm.fit(x_samples, y)
        svm_model = svm.model
        svm_intercept = svm.intercept
        svm_hyperplane = ((-svm_model[0] / svm_model[1]) * domain_x) - (svm_intercept / svm_model[1])


        # plot and save the result graphs
        # plt.plot(label_y1[0], label_y_minus1)
        plt.scatter(label_y1[:, 0], label_y1[:, 1], color='blue')
        plt.scatter(label_y_minus1[:, 0], label_y_minus1[:, 1], color='orange')
        plt.plot(domain_x, true_hyperplane, color="red")

        plt.plot(domain_x, perceptron_hyperplane, color="purple")
        plt.plot(domain_x, svm_hyperplane, color="green")
        plt.legend(['True', 'Perceptron', 'SVM', 'positive', 'negative'], bbox_to_anchor=(1.05, 1), loc='upper left',
                   borderaxespad=0.)
        plt.title("hyperplane created by the each classifier algorithm")
        plt.tight_layout()
        plt.show()

def train_and_test_accurcy():
    m = np.array([5, 10, 15, 25, 70])
    perceptron_mean_acc, svm_mean_acc, lda_mean_acc = np.array([]), np.array([]), np.array([])
    k = 10000
    for i in m:
        print(i)
        training_set , y = draw_points(i)
        perceptron_acc, svm_acc, lda_acc = 0, 0, 0
        for j in range(500):
            while (1 not in y) or (-1 not in y):
                training_set, y = draw_points(i)
            test_set, y_test = draw_points(k)

            perceptron = Perceptron()
            perceptron.fit(training_set, y)
            perceptron_acc += perceptron.score(test_set, y_test)["accuracy"]

            lda = LDA()
            lda.fit(training_set, y)
            lda_acc += lda.score(test_set, y_test)["accuracy"]

            svm = SVM()
            svm.fit(training_set, y)
            svm_acc += svm.score(test_set, y_test)["accuracy"]

        print("perceptron_acc = ", perceptron_acc)
        print("lda acc = ", lda_acc)
        print("svm acc = ", svm_acc)
        # the mean has to be in range of (0, 1) so we dived by the number of iteration 500
        perceptron_mean_acc = np.append(perceptron_mean_acc, perceptron_acc / 500)
        lda_mean_acc = np.append(lda_mean_acc, lda_acc/500)
        svm_mean_acc = np.append(svm_mean_acc, svm_acc/500)





    plt.plot(m, perceptron_mean_acc, color="purple")
    plt.plot(m, lda_mean_acc, color="red")
    plt.plot(m, svm_mean_acc, color="green")
    plt.title("accuracy for different number of samples: Perceptron, LDA, SVM ")
    plt.legend(["Perceptron", "LDA", "SVM"], bbox_to_anchor=(1.05, 1), loc='upper left',
               borderaxespad=0.)
    plt.tight_layout()
    plt.show()






if __name__ == '__main__':
    # print(draw_points(4))
    create_samples_and_models()
    # train_and_test_accurcy()
    # X = np.array([[-3, -2, 2, 3, 3, -3], [1, 1.5, -2, -2, -3, 3]]).T
    # y_1 = np.array([1, -1, -1, 1, 1, -1])